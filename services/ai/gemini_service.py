import json
import asyncio
from datetime import datetime
from typing import Dict, Any, List, Optional

import vertexai
from vertexai.generative_models import GenerativeModel, ChatSession

from config.settings import settings
from utils.logger import logger


class VertexAIModelSingleton:
    """ðŸ§  Singleton para Gemini optimizado para decisiones logÃ­sticas"""
    _model = None
    _chat_session = None

    @classmethod
    def get_model(cls) -> GenerativeModel:
        if cls._model is None:
            logger.info("ðŸ§  Inicializando Gemini 2.0 Flash para decisiones logÃ­sticas")
            vertexai.init(project=settings.PROJECT_ID, location=settings.REGION)
            cls._model = GenerativeModel(settings.MODEL_NAME)
        return cls._model

    @classmethod
    def get_chat_session(cls) -> ChatSession:
        if cls._chat_session is None:
            cls._chat_session = cls.get_model().start_chat()
        return cls._chat_session


class GeminiLogisticsDecisionEngine:
    """ðŸŽ¯ Motor de decisiÃ³n logÃ­stica con Gemini especializado en optimizaciÃ³n de rutas"""

    def __init__(self):
        self.model = VertexAIModelSingleton.get_model()
        self.decision_context = {}

    def _serialize_for_json(self, obj: Any) -> Any:
        """ðŸ”§ SerializaciÃ³n robusta para JSON"""
        if isinstance(obj, datetime):
            return obj.isoformat()
        elif isinstance(obj, dict):
            return {k: self._serialize_for_json(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._serialize_for_json(item) for item in obj]
        elif hasattr(obj, '__dict__'):
            return self._serialize_for_json(obj.__dict__)
        else:
            return obj

    async def select_optimal_route(self,
                                   top_candidates: List[Dict[str, Any]],
                                   request_context: Dict[str, Any],
                                   external_factors: Dict[str, Any]) -> Dict[str, Any]:
        """ðŸ† SelecciÃ³n final de ruta Ã³ptima por Gemini"""

        if not top_candidates:
            raise ValueError("No hay candidatos para evaluar")

        if len(top_candidates) == 1:
            return {
                'candidato_seleccionado': top_candidates[0],
                'razonamiento': 'Ãšnico candidato disponible',
                'confianza_decision': 0.8,
                'factores_decisivos': ['unica_opcion_disponible']
            }

        # Preparar contexto completo para Gemini
        context_data = self._serialize_for_json({
            'request': request_context,
            'factores_externos': external_factors,
            'candidatos': top_candidates,
            'business_rules': settings.DELIVERY_RULES,
            'weights': {
                'tiempo': settings.PESO_TIEMPO,
                'costo': settings.PESO_COSTO,
                'probabilidad': settings.PESO_PROBABILIDAD,
                'distancia': settings.PESO_DISTANCIA
            }
        })

        prompt = self._build_route_selection_prompt(context_data)

        try:
            response = await self.model.generate_content_async(prompt)
            decision = self._parse_json_response(response.text)

            # Validar que la decisiÃ³n sea vÃ¡lida
            selected_id = decision.get('candidato_seleccionado_id')
            selected_candidate = None

            for candidate in top_candidates:
                if candidate['ruta_id'] == selected_id:
                    selected_candidate = candidate
                    break

            if not selected_candidate:
                logger.warning(f"âŒ Gemini seleccionÃ³ ID invÃ¡lido: {selected_id}")
                selected_candidate = top_candidates[0]  # Fallback al mejor por score
                decision['razonamiento'] = f"Fallback al mejor candidato (ID invÃ¡lido: {selected_id})"

            decision['candidato_seleccionado'] = selected_candidate
            decision['candidatos_evaluados'] = top_candidates
            decision['timestamp_decision'] = datetime.now().isoformat()

            logger.info(f"ðŸ§  Gemini seleccionÃ³ ruta: {selected_candidate['ruta_id']}")
            return decision

        except Exception as e:
            logger.error(f"âŒ Error en decisiÃ³n Gemini: {e}")
            return self._fallback_decision(top_candidates)

    def _build_route_selection_prompt(self, context: Dict[str, Any]) -> str:
        """ðŸ”§ Construye prompt especializado para selecciÃ³n de rutas"""

        prompt = f"""
# SISTEMA EXPERTO LOGÃSTICO LIVERPOOL

Eres el sistema de decisiÃ³n mÃ¡s avanzado de Liverpool para optimizaciÃ³n de rutas de entrega. 
Tienes expertise en logÃ­stica mexicana, factores climÃ¡ticos, temporadas altas y comportamiento del consumidor.

## CONTEXTO DE LA DECISIÃ“N

**Request del Cliente:**
```json
{json.dumps(context['request'], indent=2)}
```

**Factores Externos Detectados:**
```json
{json.dumps(context['factores_externos'], indent=2)}
```

**Candidatos Top (Preseleccionados por LightGBM):**
```json
{json.dumps(context['candidatos'], indent=2)}
```

**Pesos EstratÃ©gicos Liverpool:**
- Tiempo: {context['weights']['tiempo']} (prioridad en satisfacciÃ³n cliente)
- Costo: {context['weights']['costo']} (impacto en margen)
- Probabilidad: {context['weights']['probabilidad']} (confiabilidad promesa)
- Distancia: {context['weights']['distancia']} (eficiencia operativa)

## TU MISIÃ“N

Selecciona EL MEJOR candidato considerando:

1. **Experiencia del Cliente**: Â¿CuÃ¡l cumple mejor la promesa de entrega?
2. **Eficiencia Operativa**: Â¿CuÃ¡l optimiza recursos Liverpool?
3. **GestiÃ³n de Riesgo**: Â¿CuÃ¡l minimiza probabilidad de falla?
4. **Contexto Temporal**: Â¿CÃ³mo afectan los factores externos detectados?

## REGLAS DE NEGOCIO CRÃTICAS

- Si compra antes 12:00 â†’ priorizar FLASH (mismo dÃ­a)
- Si es temporada alta (factor > 2.0) â†’ priorizar confiabilidad sobre costo
- Si es zona roja â†’ NUNCA flota interna sola
- Si producto frÃ¡gil â†’ priorizar rutas con menos transferencias
- Si lluvia > 60% â†’ penalizar rutas largas

## RESPUESTA REQUERIDA

Responde ÃšNICAMENTE en JSON vÃ¡lido:

```json
{{
    "candidato_seleccionado_id": "ID_DEL_CANDIDATO_GANADOR",
    "razonamiento": "ExplicaciÃ³n detallada de 2-3 oraciones de por quÃ© este candidato es superior, citando mÃ©tricas especÃ­ficas",
    "factores_decisivos": ["factor1", "factor2", "factor3"],
    "confianza_decision": 0.XX,
    "trade_offs_identificados": {{
        "ventajas": ["ventaja1", "ventaja2"],
        "desventajas": ["desventaja1", "desventaja2"]
    }},
    "alertas_operativas": ["alerta1", "alerta2"],
    "recomendaciones_monitoreo": ["recomendacion1", "recomendacion2"]
}}
```

## CRITERIOS DE DECISIÃ“N AVANZADOS

**Para Temporada Normal (factor â‰¤ 1.5):**
- Optimizar tiempo-costo
- Priorizar rutas directas
- Minimizar complejidad

**Para Temporada Alta (factor > 2.0):**
- Priorizar confiabilidad (probabilidad)
- Aceptar costos premium por garantÃ­a
- Evitar rutas con mÃºltiples transferencias

**Para Zona Roja:**
- OBLIGATORIO flota externa o hÃ­brida
- Verificar cobertura carrier externo
- Aumentar buffer de tiempo

**Para Clima Adverso:**
- Penalizar distancias > 100km
- Priorizar rutas urbanas
- Considerar delays adicionales

ANALIZA profundamente y decide con la expertise de 20 aÃ±os en logÃ­stica MÃ©xico.
"""

        return prompt

    async def validate_inventory_split(self,
                                       split_plan: Dict[str, Any],
                                       product_info: Dict[str, Any],
                                       request_context: Dict[str, Any]) -> Dict[str, Any]:
        """ðŸ“¦ Valida y optimiza plan de split de inventario"""

        context_data = self._serialize_for_json({
            'split_plan': split_plan,
            'producto': product_info,
            'request': request_context
        })

        prompt = f"""
# VALIDADOR DE SPLIT DE INVENTARIO LIVERPOOL

Analiza este plan de divisiÃ³n de inventario como experto en fulfillment:

## DATOS DEL SPLIT
```json
{json.dumps(context_data, indent=2)}
```

## EVALÃšA

1. **Viabilidad Operativa**: Â¿Es ejecutable en la prÃ¡ctica?
2. **Eficiencia de Costos**: Â¿Justifica la complejidad adicional?
3. **Experiencia Cliente**: Â¿Afecta la promesa de entrega?
4. **Riesgo Operativo**: Â¿QuÃ© puede fallar?

## ALTERNATIVAS A CONSIDERAR

- Â¿Consolidar todo desde una ubicaciÃ³n es mejor?
- Â¿El split agrega valor real al cliente?
- Â¿Los tiempos de preparaciÃ³n son realistas?

Responde en JSON:

```json
{{
    "split_recomendado": true/false,
    "justificacion": "Razones especÃ­ficas",
    "optimizaciones": ["optimizacion1", "optimizacion2"],
    "riesgos_identificados": ["riesgo1", "riesgo2"],
    "alternativa_sugerida": "DescripciÃ³n si split no es Ã³ptimo",
    "score_viabilidad": 0.XX
}}
```
        """

        try:
            response = await self.model.generate_content_async(prompt)
            return self._parse_json_response(response.text)
        except Exception as e:
            logger.error(f"âŒ Error validando split: {e}")
            return {
                'split_recomendado': split_plan.get('es_factible', False),
                'justificacion': 'ValidaciÃ³n automÃ¡tica por error en Gemini',
                'score_viabilidad': 0.7
            }

    async def analyze_external_factors_impact(self,
                                              external_factors: Dict[str, Any],
                                              target_postal_code: str,
                                              delivery_date: datetime) -> Dict[str, Any]:
        """ðŸŒ¤ï¸ Analiza impacto de factores externos en la entrega"""

        context_data = self._serialize_for_json({
            'factores': external_factors,
            'codigo_postal': target_postal_code,
            'fecha_entrega': delivery_date,
            'fecha_actual': datetime.now()
        })

        prompt = f"""
# ANÃLISIS DE FACTORES EXTERNOS MÃ‰XICO

Como experto en logÃ­stica mexicana, analiza el impacto de estos factores:

```json
{json.dumps(context_data, indent=2)}
```

## CONTEXTO MÃ‰XICO

- Temporadas: Buen Fin (Nov), Navidad (Dic), DÃ­a Madres (May)
- Clima: Temporada lluvia Jun-Sep
- TrÃ¡fico: CDMX crÃ­tico 7-10am, 6-9pm
- Zonas: Norte mÃ¡s seguro, Sur mÃ¡s complicado

## ANALIZA

1. **Impacto Temporal**: Â¿CÃ³mo afectan los tiempos?
2. **Impacto EconÃ³mico**: Â¿Aumentan los costos?
3. **Riesgo Operativo**: Â¿QuÃ© probabilidad de falla?
4. **MitigaciÃ³n**: Â¿QuÃ© acciones tomar?

Responde en JSON:

```json
{{
    "impacto_tiempo_horas": X.X,
    "impacto_costo_pct": X.X,
    "probabilidad_retraso": 0.XX,
    "criticidad": "Baja|Media|Alta|CrÃ­tica",
    "factores_criticos": ["factor1", "factor2"],
    "estrategias_mitigacion": ["estrategia1", "estrategia2"],
    "alertas_especiales": ["alerta1", "alerta2"],
    "recomendacion_flota": "FI|FE|FI_FE"
}}
```
        """

        try:
            response = await self.model.generate_content_async(prompt)
            return self._parse_json_response(response.text)
        except Exception as e:
            logger.error(f"âŒ Error analizando factores: {e}")
            return {
                'impacto_tiempo_horas': 0.5,
                'impacto_costo_pct': 5.0,
                'criticidad': 'Media',
                'factores_criticos': ['error_gemini']
            }

    async def generate_final_explanation(self,
                                         selected_route: Dict[str, Any],
                                         all_context: Dict[str, Any]) -> Dict[str, Any]:
        """ðŸ“Š Genera explicaciÃ³n ejecutiva completa"""

        context_data = self._serialize_for_json({
            'ruta_seleccionada': selected_route,
            'contexto_completo': all_context
        })

        prompt = f"""
# EXPLICACIÃ“N EJECUTIVA LIVERPOOL FEE

Genera un resumen ejecutivo de esta decisiÃ³n logÃ­stica para stakeholders:

```json
{json.dumps(context_data, indent=2)}
```

## AUDIENCIA
- Gerentes de operaciones
- Customer service
- Equipos de fulfillment

## INCLUYE

1. **Resumen de 1 lÃ­nea**: La decisiÃ³n principal
2. **JustificaciÃ³n**: Por quÃ© es la mejor opciÃ³n
3. **MÃ©tricas clave**: Tiempo, costo, probabilidad
4. **Factores considerados**: QuÃ© influyÃ³ en la decisiÃ³n
5. **Acciones requeridas**: QuÃ© debe hacer el equipo operativo
6. **Monitoreo**: QuÃ© vigilar durante la ejecuciÃ³n

Responde en JSON:

```json
{{
    "resumen_ejecutivo": "Una lÃ­nea describiendo la decisiÃ³n",
    "valor_cliente": "CÃ³mo beneficia al cliente",
    "eficiencia_operativa": "Impacto en operaciones",
    "metricas_clave": {{
        "tiempo_entrega": "X horas",
        "costo_total": "$X MXN",
        "confiabilidad": "XX%"
    }},
    "factores_determinantes": ["factor1", "factor2"],
    "acciones_operativas": ["accion1", "accion2"],
    "kpis_monitoreo": ["kpi1", "kpi2"],
    "nivel_confianza": "Alto|Medio|Bajo",
    "proxima_revision": "CuÃ¡ndo revisar la predicciÃ³n"
}}
```
        """

        try:
            response = await self.model.generate_content_async(prompt)
            return self._parse_json_response(response.text)
        except Exception as e:
            logger.error(f"âŒ Error generando explicaciÃ³n: {e}")
            return {
                'resumen_ejecutivo': 'Ruta optimizada seleccionada automÃ¡ticamente',
                'nivel_confianza': 'Medio'
            }

    def _parse_json_response(self, response_text: str) -> Dict[str, Any]:
        """ðŸ”§ Parser robusto mejorado para respuestas JSON de Gemini"""

        try:
            # Limpiar markdown
            clean_text = response_text.strip()

            # Remover markdown blocks
            if "```json" in clean_text:
                clean_text = clean_text.split("```json")[1].split("```")[0]
            elif "```" in clean_text:
                # Manejar cases donde no hay 'json' especificado
                parts = clean_text.split("```")
                for part in parts:
                    if "{" in part and "}" in part:
                        clean_text = part
                        break

            # Encontrar el JSON vÃ¡lido mÃ¡s largo
            start_pos = clean_text.find("{")
            if start_pos == -1:
                raise json.JSONDecodeError("No JSON found", clean_text, 0)

            # Buscar el } que cierra correctamente
            brace_count = 0
            end_pos = start_pos

            for i, char in enumerate(clean_text[start_pos:], start_pos):
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        end_pos = i + 1
                        break

            json_text = clean_text[start_pos:end_pos]

            # Intentar parsear
            parsed = json.loads(json_text)
            logger.debug("âœ… JSON parseado correctamente de Gemini")
            return parsed

        except (json.JSONDecodeError, IndexError, AttributeError) as e:
            logger.error(f"âŒ Error parsing JSON de Gemini: {e}")
            logger.error(f"Texto recibido: {response_text[:300]}...")

            # Fallback response mÃ¡s robusto
            return {
                "error": "JSON parsing failed",
                "candidato_seleccionado_id": "fallback",
                "razonamiento": "Error en parsing - selecciÃ³n automÃ¡tica",
                "confianza_decision": 0.5,
                "factores_decisivos": ["error_parsing"],
                "split_recomendado": True,
                "justificacion": "Fallback por error de parsing",
                "score_viabilidad": 0.6,
                "impacto_tiempo_horas": 1.0,
                "impacto_costo_pct": 10.0,
                "criticidad": "Media",
                "resumen_ejecutivo": "DecisiÃ³n automÃ¡tica por error en IA"
            }

    def _fallback_decision(self, candidates: List[Dict[str, Any]]) -> Dict[str, Any]:
        """ðŸ”„ DecisiÃ³n fallback cuando Gemini falla"""

        if not candidates:
            raise ValueError("No hay candidatos para fallback")

        # Seleccionar el mejor por score LightGBM
        best_candidate = max(candidates, key=lambda x: x.get('score_lightgbm', 0))

        return {
            'candidato_seleccionado': best_candidate,
            'candidatos_evaluados': candidates,
            'razonamiento': 'SelecciÃ³n automÃ¡tica por score LightGBM (fallback)',
            'confianza_decision': 0.75,
            'factores_decisivos': ['score_lightgbm', 'fallback_system'],
            'trade_offs_identificados': {
                'ventajas': ['mejor_score_ml'],
                'desventajas': ['sin_analisis_gemini']
            },
            'alertas_operativas': ['decision_fallback'],
            'timestamp_decision': datetime.now().isoformat()
        }