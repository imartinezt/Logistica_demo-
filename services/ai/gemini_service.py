import json
import asyncio
from datetime import datetime
from typing import Dict, Any, List, Optional

import vertexai
from vertexai.generative_models import GenerativeModel, ChatSession

from config.settings import settings
from utils.logger import logger


class VertexAIModelSingleton:
    """üß† Singleton para Gemini optimizado para decisiones log√≠sticas"""
    _model = None
    _chat_session = None

    @classmethod
    def get_model(cls) -> GenerativeModel:
        if cls._model is None:
            logger.info("üß† Inicializando Gemini 2.0 Flash para decisiones log√≠sticas")
            vertexai.init(project=settings.PROJECT_ID, location=settings.REGION)
            cls._model = GenerativeModel(settings.MODEL_NAME)
        return cls._model

    @classmethod
    def get_chat_session(cls) -> ChatSession:
        if cls._chat_session is None:
            cls._chat_session = cls.get_model().start_chat()
        return cls._chat_session


class GeminiLogisticsDecisionEngine:
    """üéØ Motor de decisi√≥n log√≠stica con Gemini especializado en optimizaci√≥n de rutas"""

    def __init__(self):
        self.model = VertexAIModelSingleton.get_model()
        self.decision_context = {}

    def _serialize_for_json(self, obj: Any) -> Any:
        """üîß Serializaci√≥n robusta para JSON"""
        if isinstance(obj, datetime):
            return obj.isoformat()
        elif isinstance(obj, dict):
            return {k: self._serialize_for_json(v) for k, v in obj.items()}
        elif isinstance(obj, list):
            return [self._serialize_for_json(item) for item in obj]
        elif hasattr(obj, '__dict__'):
            return self._serialize_for_json(obj.__dict__)
        else:
            return obj

    async def select_optimal_route(self,
                                   top_candidates: List[Dict[str, Any]],
                                   request_context: Dict[str, Any],
                                   external_factors: Dict[str, Any]) -> Dict[str, Any]:
        """üèÜ Selecci√≥n final de ruta √≥ptima por Gemini"""

        if not top_candidates:
            raise ValueError("No hay candidatos para evaluar")

        if len(top_candidates) == 1:
            return {
                'candidato_seleccionado': top_candidates[0],
                'razonamiento': '√önico candidato disponible',
                'confianza_decision': 0.8,
                'factores_decisivos': ['unica_opcion_disponible']
            }

        # Preparar contexto completo para Gemini
        context_data = self._serialize_for_json({
            'request': request_context,
            'factores_externos': external_factors,
            'candidatos': top_candidates,
            'business_rules': settings.DELIVERY_RULES,
            'weights': {
                'tiempo': settings.PESO_TIEMPO,
                'costo': settings.PESO_COSTO,
                'probabilidad': settings.PESO_PROBABILIDAD,
                'distancia': settings.PESO_DISTANCIA
            }
        })

        prompt = self._build_route_selection_prompt(context_data)

        try:
            response = await self.model.generate_content_async(prompt)
            decision = self._parse_json_response(response.text)

            # Validar que la decisi√≥n sea v√°lida
            selected_id = decision.get('candidato_seleccionado_id')
            selected_candidate = None

            for candidate in top_candidates:
                if candidate['ruta_id'] == selected_id:
                    selected_candidate = candidate
                    break

            if not selected_candidate:
                logger.warning(f"‚ùå Gemini seleccion√≥ ID inv√°lido: {selected_id}")
                selected_candidate = top_candidates[0]  # Fallback al mejor por score
                decision['razonamiento'] = f"Fallback al mejor candidato (ID inv√°lido: {selected_id})"

            decision['candidato_seleccionado'] = selected_candidate
            decision['candidatos_evaluados'] = top_candidates
            decision['timestamp_decision'] = datetime.now().isoformat()

            logger.info(f"üß† Gemini seleccion√≥ ruta: {selected_candidate['ruta_id']}")
            return decision

        except Exception as e:
            logger.error(f"‚ùå Error en decisi√≥n Gemini: {e}")
            return self._fallback_decision(top_candidates)

    def _build_route_selection_prompt(self, context: Dict[str, Any]) -> str:
        """üîß Construye prompt especializado para selecci√≥n de rutas"""

        prompt = f"""
# SISTEMA EXPERTO LOG√çSTICO LIVERPOOL

Eres el sistema de decisi√≥n m√°s avanzado de Liverpool para optimizaci√≥n de rutas de entrega. 
Tienes expertise en log√≠stica mexicana, factores clim√°ticos, temporadas altas y comportamiento del consumidor.

## CONTEXTO DE LA DECISI√ìN

**Request del Cliente:**
```json
{json.dumps(context['request'], indent=2)}
```

**Factores Externos Detectados:**
```json
{json.dumps(context['factores_externos'], indent=2)}
```

**Candidatos Top (Preseleccionados por LightGBM):**
```json
{json.dumps(context['candidatos'], indent=2)}
```

**Pesos Estrat√©gicos Liverpool:**
- Tiempo: {context['weights']['tiempo']} (prioridad en satisfacci√≥n cliente)
- Costo: {context['weights']['costo']} (impacto en margen)
- Probabilidad: {context['weights']['probabilidad']} (confiabilidad promesa)
- Distancia: {context['weights']['distancia']} (eficiencia operativa)

## TU MISI√ìN

Selecciona EL MEJOR candidato considerando:

1. **Experiencia del Cliente**: ¬øCu√°l cumple mejor la promesa de entrega?
2. **Eficiencia Operativa**: ¬øCu√°l optimiza recursos Liverpool?
3. **Gesti√≥n de Riesgo**: ¬øCu√°l minimiza probabilidad de falla?
4. **Contexto Temporal**: ¬øC√≥mo afectan los factores externos detectados?

## REGLAS DE NEGOCIO CR√çTICAS

- Si compra antes 12:00 ‚Üí priorizar FLASH (mismo d√≠a)
- Si es temporada alta (factor > 2.0) ‚Üí priorizar confiabilidad sobre costo
- Si es zona roja ‚Üí NUNCA flota interna sola
- Si producto fr√°gil ‚Üí priorizar rutas con menos transferencias
- Si lluvia > 60% ‚Üí penalizar rutas largas

## RESPUESTA REQUERIDA

Responde √öNICAMENTE en JSON v√°lido:

```json
{{
    "candidato_seleccionado_id": "ID_DEL_CANDIDATO_GANADOR",
    "razonamiento": "Explicaci√≥n detallada de 2-3 oraciones de por qu√© este candidato es superior, citando m√©tricas espec√≠ficas",
    "factores_decisivos": ["factor1", "factor2", "factor3"],
    "confianza_decision": 0.XX,
    "trade_offs_identificados": {{
        "ventajas": ["ventaja1", "ventaja2"],
        "desventajas": ["desventaja1", "desventaja2"]
    }},
    "alertas_operativas": ["alerta1", "alerta2"],
    "recomendaciones_monitoreo": ["recomendacion1", "recomendacion2"]
}}
```

## CRITERIOS DE DECISI√ìN AVANZADOS

**Para Temporada Normal (factor ‚â§ 1.5):**
- Optimizar tiempo-costo
- Priorizar rutas directas
- Minimizar complejidad

**Para Temporada Alta (factor > 2.0):**
- Priorizar confiabilidad (probabilidad)
- Aceptar costos premium por garant√≠a
- Evitar rutas con m√∫ltiples transferencias

**Para Zona Roja:**
- OBLIGATORIO flota externa o h√≠brida
- Verificar cobertura carrier externo
- Aumentar buffer de tiempo

**Para Clima Adverso:**
- Penalizar distancias > 100km
- Priorizar rutas urbanas
- Considerar delays adicionales

ANALIZA profundamente y decide con la expertise de 20 a√±os en log√≠stica M√©xico.
"""

        return prompt

    async def validate_inventory_split(self,
                                       split_plan: Dict[str, Any],
                                       product_info: Dict[str, Any],
                                       request_context: Dict[str, Any]) -> Dict[str, Any]:
        """üì¶ Valida y optimiza plan de split de inventario"""

        context_data = self._serialize_for_json({
            'split_plan': split_plan,
            'producto': product_info,
            'request': request_context
        })

        prompt = f"""
# VALIDADOR DE SPLIT DE INVENTARIO LIVERPOOL

Analiza este plan de divisi√≥n de inventario como experto en fulfillment:

## DATOS DEL SPLIT
```json
{json.dumps(context_data, indent=2)}
```

## EVAL√öA

1. **Viabilidad Operativa**: ¬øEs ejecutable en la pr√°ctica?
2. **Eficiencia de Costos**: ¬øJustifica la complejidad adicional?
3. **Experiencia Cliente**: ¬øAfecta la promesa de entrega?
4. **Riesgo Operativo**: ¬øQu√© puede fallar?

## ALTERNATIVAS A CONSIDERAR

- ¬øConsolidar todo desde una ubicaci√≥n es mejor?
- ¬øEl split agrega valor real al cliente?
- ¬øLos tiempos de preparaci√≥n son realistas?

Responde en JSON:

```json
{{
    "split_recomendado": true/false,
    "justificacion": "Razones espec√≠ficas",
    "optimizaciones": ["optimizacion1", "optimizacion2"],
    "riesgos_identificados": ["riesgo1", "riesgo2"],
    "alternativa_sugerida": "Descripci√≥n si split no es √≥ptimo",
    "score_viabilidad": 0.XX
}}
```
        """

        try:
            response = await self.model.generate_content_async(prompt)
            return self._parse_json_response(response.text)
        except Exception as e:
            logger.error(f"‚ùå Error validando split: {e}")
            return {
                'split_recomendado': split_plan.get('es_factible', False),
                'justificacion': 'Validaci√≥n autom√°tica por error en Gemini',
                'score_viabilidad': 0.7
            }

    async def analyze_external_factors_impact(self,
                                              external_factors: Dict[str, Any],
                                              target_postal_code: str,
                                              delivery_date: datetime) -> Dict[str, Any]:
        """üå§Ô∏è Analiza impacto de factores externos en la entrega"""

        context_data = self._serialize_for_json({
            'factores': external_factors,
            'codigo_postal': target_postal_code,
            'fecha_entrega': delivery_date,
            'fecha_actual': datetime.now()
        })

        prompt = f"""
# AN√ÅLISIS DE FACTORES EXTERNOS M√âXICO

Como experto en log√≠stica mexicana, analiza el impacto de estos factores:

```json
{json.dumps(context_data, indent=2)}
```

## CONTEXTO M√âXICO

- Temporadas: Buen Fin (Nov), Navidad (Dic), D√≠a Madres (May)
- Clima: Temporada lluvia Jun-Sep
- Tr√°fico: CDMX cr√≠tico 7-10am, 6-9pm
- Zonas: Norte m√°s seguro, Sur m√°s complicado

## ANALIZA

1. **Impacto Temporal**: ¬øC√≥mo afectan los tiempos?
2. **Impacto Econ√≥mico**: ¬øAumentan los costos?
3. **Riesgo Operativo**: ¬øQu√© probabilidad de falla?
4. **Mitigaci√≥n**: ¬øQu√© acciones tomar?

Responde en JSON:

```json
{{
    "impacto_tiempo_horas": X.X,
    "impacto_costo_pct": X.X,
    "probabilidad_retraso": 0.XX,
    "criticidad": "Baja|Media|Alta|Cr√≠tica",
    "factores_criticos": ["factor1", "factor2"],
    "estrategias_mitigacion": ["estrategia1", "estrategia2"],
    "alertas_especiales": ["alerta1", "alerta2"],
    "recomendacion_flota": "FI|FE|FI_FE"
}}
```
        """

        try:
            response = await self.model.generate_content_async(prompt)
            return self._parse_json_response(response.text)
        except Exception as e:
            logger.error(f"‚ùå Error analizando factores: {e}")
            return {
                'impacto_tiempo_horas': 0.5,
                'impacto_costo_pct': 5.0,
                'criticidad': 'Media',
                'factores_criticos': ['error_gemini']
            }

    async def generate_final_explanation(self,
                                         selected_route: Dict[str, Any],
                                         all_context: Dict[str, Any]) -> Dict[str, Any]:
        """üìä Genera explicaci√≥n ejecutiva completa"""

        context_data = self._serialize_for_json({
            'ruta_seleccionada': selected_route,
            'contexto_completo': all_context
        })

        prompt = f"""
# EXPLICACI√ìN EJECUTIVA LIVERPOOL FEE

Genera un resumen ejecutivo de esta decisi√≥n log√≠stica para stakeholders:

```json
{json.dumps(context_data, indent=2)}
```

## AUDIENCIA
- Gerentes de operaciones
- Customer service
- Equipos de fulfillment

## INCLUYE

1. **Resumen de 1 l√≠nea**: La decisi√≥n principal
2. **Justificaci√≥n**: Por qu√© es la mejor opci√≥n
3. **M√©tricas clave**: Tiempo, costo, probabilidad
4. **Factores considerados**: Qu√© influy√≥ en la decisi√≥n
5. **Acciones requeridas**: Qu√© debe hacer el equipo operativo
6. **Monitoreo**: Qu√© vigilar durante la ejecuci√≥n

Responde en JSON:

```json
{{
    "resumen_ejecutivo": "Una l√≠nea describiendo la decisi√≥n",
    "valor_cliente": "C√≥mo beneficia al cliente",
    "eficiencia_operativa": "Impacto en operaciones",
    "metricas_clave": {{
        "tiempo_entrega": "X horas",
        "costo_total": "$X MXN",
        "confiabilidad": "XX%"
    }},
    "factores_determinantes": ["factor1", "factor2"],
    "acciones_operativas": ["accion1", "accion2"],
    "kpis_monitoreo": ["kpi1", "kpi2"],
    "nivel_confianza": "Alto|Medio|Bajo",
    "proxima_revision": "Cu√°ndo revisar la predicci√≥n"
}}
```
        """

        try:
            response = await self.model.generate_content_async(prompt)
            return self._parse_json_response(response.text)
        except Exception as e:
            logger.error(f"‚ùå Error generando explicaci√≥n: {e}")
            return {
                'resumen_ejecutivo': 'Ruta optimizada seleccionada autom√°ticamente',
                'nivel_confianza': 'Medio'
            }

    def _parse_json_response(self, response_text: str) -> Dict[str, Any]:
        """üîß Parser robusto mejorado para respuestas JSON de Gemini"""

        try:
            # Limpiar markdown
            clean_text = response_text.strip()

            # Remover markdown blocks
            if "```json" in clean_text:
                clean_text = clean_text.split("```json")[1].split("```")[0]
            elif "```" in clean_text:
                # Manejar cases donde no hay 'json' especificado
                parts = clean_text.split("```")
                for part in parts:
                    if "{" in part and "}" in part:
                        clean_text = part
                        break

            # Encontrar el JSON v√°lido m√°s largo
            start_pos = clean_text.find("{")
            if start_pos == -1:
                raise json.JSONDecodeError("No JSON found", clean_text, 0)

            # Buscar el } que cierra correctamente
            brace_count = 0
            end_pos = start_pos

            for i, char in enumerate(clean_text[start_pos:], start_pos):
                if char == '{':
                    brace_count += 1
                elif char == '}':
                    brace_count -= 1
                    if brace_count == 0:
                        end_pos = i + 1
                        break

            json_text = clean_text[start_pos:end_pos]

            # Intentar parsear
            parsed = json.loads(json_text)
            logger.debug("‚úÖ JSON parseado correctamente de Gemini")
            return parsed

        except (json.JSONDecodeError, IndexError, AttributeError) as e:
            logger.error(f"‚ùå Error parsing JSON de Gemini: {e}")
            logger.error(f"Texto recibido: {response_text[:300]}...")

            # Fallback response m√°s robusto
            return {
                "error": "JSON parsing failed",
                "candidato_seleccionado_id": "fallback",
                "razonamiento": "Error en parsing - selecci√≥n autom√°tica",
                "confianza_decision": 0.5,
                "factores_decisivos": ["error_parsing"],
                "split_recomendado": True,
                "justificacion": "Fallback por error de parsing",
                "score_viabilidad": 0.6,
                "impacto_tiempo_horas": 1.0,
                "impacto_costo_pct": 10.0,
                "criticidad": "Media",
                "resumen_ejecutivo": "Decisi√≥n autom√°tica por error en IA"
            }

    def _fallback_decision(self, candidates: List[Dict[str, Any]]) -> Dict[str, Any]:
        """üîÑ Decisi√≥n fallback cuando Gemini falla"""

        if not candidates:
            raise ValueError("No hay candidatos para fallback")

        # Seleccionar el mejor por score LightGBM
        best_candidate = max(candidates, key=lambda x: x.get('score_lightgbm', 0))

        return {
            'candidato_seleccionado': best_candidate,
            'candidatos_evaluados': candidates,
            'razonamiento': 'Selecci√≥n autom√°tica por score LightGBM (fallback)',
            'confianza_decision': 0.75,
            'factores_decisivos': ['score_lightgbm', 'fallback_system'],
            'trade_offs_identificados': {
                'ventajas': ['mejor_score_ml'],
                'desventajas': ['sin_analisis_gemini']
            },
            'alertas_operativas': ['decision_fallback'],
            'timestamp_decision': datetime.now().isoformat()
        }